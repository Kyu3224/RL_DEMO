{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# <center><b>Reinforcement Learning for Quadruped Locomotion</b></center>\n\n<div align=\"center\">\n\n<h3>KAIST DRCD Lab</h3>\n\n\n<b>Instructor</b><br>\n<a href=\"https://dynamicrobot.kaist.ac.kr/people.html\">Hae-Won Park</a>\n(haewonpark [at] kaist.ac.kr)\n\n<br>\n\n<b>Teaching Assistants</b><br>\n<a href=\"https://kdyun0118.github.io/\">Dongyun Kang</a>\n(kdong7309 [at] kaist.ac.kr)<br>\n<a href=\"https://github.com/parkjahun42\">Jaehyun Park</a>\n(parkjahun42 [at] kaist.ac.kr)<br>\n<a href=\"https://github.com/sowoolee\">Sowoo Lee</a>\n(dlthdn [at] kaist.ac.kr)\n\n\n<video width=\"900\" controls autoplay loop>\n  <source src=\"../images/dummy_video.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n<br>\n\n<img src=\"../images/dummy_image.png\" width=\"900\">\n\n\n\n\n</div>\n\n<br>\n<br>\n\n## Abstract\n\n이 튜토리얼은 **MuJoCo 시뮬레이터 환경에서 사족보행 로봇(Go1)의 강화학습을 수행**하는 것을 목표로 합니다.\nPPO(Proximal Policy Optimization)를 기반으로 한 보행 정책 학습 과정을 단계별로 설명하며,\n\n- Go1 MuJoCo 환경 구성 및 관측/행동 설계\n- PPO 학습 파이프라인과 주요 하이퍼파라미터\n- 사전 학습된 정책(pretrained policy)을 활용한 파인튜닝\n- Reward ablation을 통한 보행 성능 비교 분석\n\n을 포함합니다.\n\n본 튜토리얼은 **강화학습 기반 로봇 제어를 처음 접하는 학생부터, 실제 보행 정책을 분석하고자 하는 연구자**까지를 대상으로 설계되었습니다.\n\n<br>\n\n<center>\n\n**세부 실험 구성 및 확장 가이드는 노트북 하단에 제공됩니다.**\n\n</center>\n\n<br>\n\n---",
   "id": "1ebddf348b12cf59"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Environment Setup\n",
    "\n",
    "이 섹션에서는 튜토리얼 실행을 위한 환경을 설정합니다.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1tamjiJCNrQ5W-3KFr6JAoILyXDLMHoPQ\"\n",
    "       width=\"600\">\n",
    "</div>\n",
    "\n",
    "본 튜토리얼은 **Google Colab** 환경을 기준으로 작성되었습니다.\n",
    "아래 순서에 따라 실행 환경을 준비해 주세요.\n",
    "\n",
    "1. **런타임 유형을 GPU(T4)로 변경**하고 런타임에 연결합니다.\n",
    "2. 제공된 GitHub 레포지토리를 **clone**하여 코드 베이스를 준비합니다.\n",
    "3. MuJoCo 및 강화학습 실험에 필요한 **의존성 패키지들을 설치**합니다.\n"
   ],
   "id": "f725a308b7e9dcdb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Clone repository\n",
    "import os, sys\n",
    "\n",
    "import yaml\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "if not os.path.isdir(\"RL_DEMO\"):\n",
    "  !git clone https://github.com/DrcdKAIST/RL_DEMO.git\n",
    "else:\n",
    "  print(\"Cloned Directory already exists\")\n",
    "\n",
    "os.chdir(\"/content/RL_DEMO\")\n",
    "print(\"Current Directory: \", os.getcwd())\n",
    "\n",
    "sys.path.insert(0, \"/content/RL_DEMO\")\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy tensorboard gymnasium==0.29.1 stable-baselines3==2.3.0 mujoco==3.1.5 imageio"
   ],
   "id": "34d21ce7d64e751c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Go1 MuJoCo Environment Description\n",
    "\n",
    "본 튜토리얼에서는 **MuJoCo 기반 Go1 사족보행 로봇 환경**에서 강화학습을 수행합니다.\n",
    "\n",
    "학습 환경은 `Go1MujocoEnv` 클래스로 구현되어 있으며, 로봇이 지정된 이동 command를 안정적으로 추종하는 과제를 **마르코프 결정 과정(Markov Decision Process, MDP)** 으로 구성합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Simulation Setup\n",
    "\n",
    "- **물리 시뮬레이터**: MuJoCo\n",
    "- **로봇 모델**: Unitree Go1\n",
    "- **시뮬레이션 정의**:\n",
    "  시뮬레이션 환경은 `unitree_go1/scene_position.xml` 파일에 정의되어 있으며,\n",
    "  로봇 모델, 지면, 조명, 카메라 설정을 포함합니다.\n",
    "- **로봇 설정**:\n",
    "  관절 액추에이터, PD 제어 이득, 기구학 구조는 `go1_position.xml` 파일에 정의되어 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Environment Configuration\n",
    "\n",
    "강화학습 환경의 로직은 다음 파일들에 구현되어 있습니다.\n",
    "\n",
    "- `src/go1_mujoco_env.py` — 환경의 동역학 및 MDP 구성 로직\n",
    "- `src/envs.yaml` — 환경 설정 파일로, 다음 항목들을 포함합니다.\n",
    "  - 에피소드 길이\n",
    "  - Reward 가중치\n",
    "  - 목표 속도(command) 범위\n",
    "  - Observation 스케일링\n",
    "  - 조기 종료(early termination) 조건\n",
    "\n",
    "<br>\n"
   ],
   "id": "6c9964167fc68c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Environment configuration\n",
    "!sed -n '1,200p' src/envs.yaml"
   ],
   "id": "9b7ea2df62b99c0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Environment Step Loop\n",
    "\n",
    "각 타임스텝마다 다음과 같은 절차가 수행됩니다.\n",
    "\n",
    "1. 정책(policy)이 현재 상태를 기반으로 행동(action)을 출력합니다.\n",
    "2. 시뮬레이터가 고정된 프레임 수(`frame_skip`)만큼 진행됩니다.\n",
    "3. Observation, reward, 종료 조건이 계산됩니다.\n",
    "4. 해당 전이(transition)가 학습을 위해 저장됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "Observation 벡터는 다음과 같은 정보를 포함합니다.\n",
    "\n",
    "- 로봇 베이스의 선형 및 각속도\n",
    "- 중력 방향이 투영된 벡터\n",
    "- 목표 이동 속도(command)\n",
    "- 관절 위치 및 속도\n",
    "- 이전 타임스텝의 action (제어 입력의 연속성 확보 목적)\n",
    "\n",
    "모든 observation은 학습 안정성을 위해 사전에 정의된 범위로 스케일링 및 클리핑됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Reward and Termination\n",
    "\n",
    "Reward 함수와 종료 조건은 다음 모듈에 분리되어 구현되어 있습니다.\n",
    "\n",
    "- `src/mdp/reward.py`\n",
    "- `src/mdp/termination.py`\n",
    "\n",
    "이와 같은 모듈화된 구조를 통해 개별 reward 항을 손쉽게 수정하거나 제거할 수 있으며,\n",
    "이는 튜토리얼 후반부에서 수행할 **reward ablation 실험**에서 활용됩니다."
   ],
   "id": "a65879346d5afe69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check observation / action space\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import src.go1_mujoco_env as go1_env\n",
    "\n",
    "importlib.reload(go1_env)\n",
    "\n",
    "# Create environment (no rendering)\n",
    "env = go1_env.Go1MujocoEnv(\n",
    "    prj_path=\"/content/RL_DEMO\",\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(f\"Observation shape: {np.array(obs).shape}\\n\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "print(f\"Observation space: {env.observation_space}\\n\")\n",
    "\n",
    "# Take one random step\n",
    "action = env.action_space.sample()\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"One-step reward:\", reward)\n",
    "print(\"Terminated:\", terminated)\n",
    "\n",
    "env.close()"
   ],
   "id": "a7eb798123df7d07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import imageio\n",
    "import torch\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from IPython.display import Video, display\n",
    "from pathlib import Path\n",
    "\n",
    "import src.go1_mujoco_env as go1_env\n",
    "\n",
    "from src.utils.reward_logging_callback import RewardLoggingCallback\n",
    "\n",
    "DEFAULT_CAMERA_CONFIG = {\n",
    "    \"azimuth\": 90.0,\n",
    "    \"distance\": 3.0,\n",
    "    \"elevation\": -25.0,\n",
    "    \"lookat\": np.array([0., 0., 0.]),\n",
    "    \"fixedcamid\": 0,\n",
    "    \"trackbodyid\": -1,\n",
    "    \"type\": 2,\n",
    "}\n",
    "\n",
    "policy_cfg_path = Path(\"/content/RL_DEMO/src/params.yaml\")\n",
    "with policy_cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    policy_cfg = yaml.safe_load(f)"
   ],
   "id": "db38547bbadbf940"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. PPO Code Review\n",
    "\n",
    "### 2.1 Stable-Baselines3\n",
    "\n",
    "본 튜토리얼에서는 **Stable-Baselines3(SB3)** 라이브러리에 구현된\n",
    "**PPO(Proximal Policy Optimization)** 알고리즘을 사용합니다.\n",
    "\n",
    "SB3는 PyTorch 기반의 강화학습 라이브러리로,\n",
    "PPO, SAC, TD3 등 다양한 알고리즘을 안정적으로 제공합니다.\n",
    "\n",
    "PPO는 actor-critic 구조를 기반으로 하며,\n",
    "정책 업데이트 시 변화 폭을 제한(clipping)하여\n",
    "학습 안정성을 확보하는 것이 특징입니다.\n",
    "\n",
    "이 튜토리얼에서는 PPO의 수식적 유도보다는,\n",
    "SB3에서 PPO가 어떻게 사용되는지에 초점을 맞춥니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.2 Vectorized Environments\n",
    "\n",
    "PPO는 on-policy 알고리즘이므로,\n",
    "매 업데이트마다 많은 샘플을 필요로 합니다.\n",
    "\n",
    "이를 위해 SB3는 여러 개의 환경을 동시에 실행하는\n",
    "**vectorized environment**를 지원합니다.\n",
    "\n",
    "본 튜토리얼에서는 `SubprocVecEnv`를 사용하여\n",
    "여러 개의 Go1 환경을 병렬로 실행합니다.\n",
    "이는 MuJoCo 기반 시뮬레이션에서 샘플 수집 속도를\n",
    "크게 향상시켜 줍니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.3 PPO 에이전트 생성\n",
    "\n",
    "이제 병렬 환경을 기반으로 PPO 에이전트를 생성합니다.\n",
    "\n",
    "본 튜토리얼에서는 두 가지 학습 방식을 지원합니다.\n",
    "\n",
    "- 사전 학습된 policy를 불러와 추가 학습(finetuning)\n",
    "- 네트워크를 새로 초기화하여 처음부터 학습\n",
    "\n",
    "PPO의 정책 네트워크(actor)와 가치 함수(critic)는\n",
    "SB3 내부에서 자동으로 생성됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.4 PPO 학습 루프\n",
    "\n",
    "PPO 에이전트가 생성되면,\n",
    "`learn()` 함수를 통해 학습을 수행합니다.\n",
    "\n",
    "`learn()` 내부에서는 다음 과정이 반복됩니다.\n",
    "\n",
    "1. 현재 정책으로 병렬 환경과 상호작용하며 rollout 수집\n",
    "2. GAE를 이용한 advantage 계산\n",
    "3. 정책 및 가치 함수 업데이트\n",
    "4. 학습 로그 기록 및 콜백 실행"
   ],
   "id": "6b357241f3a93e8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import stable_baselines3\n",
    "import stable_baselines3.ppo.ppo as ppo_module\n",
    "import inspect, os\n",
    "\n",
    "print(\"stable-baselines3 version:\", stable_baselines3.__version__)\n",
    "print(\"PPO module path:\", os.path.abspath(ppo_module.__file__))\n",
    "print(\"PPO class:\", ppo_module.PPO)"
   ],
   "id": "58f8dff6d3dd5d46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"https://drive.google.com/uc?id=1jm4dvSTDlkRLh4HcoiYA-y8PsIiRuhui\">\n",
    "</div>"
   ],
   "id": "3831cb890ee0773"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training and Finetuning\n",
    "\n",
    "Training quadruped locomotion policies from scratch can be time-consuming.\n",
    "Therefore, this tutorial uses a **pretrained policy** as a starting point and\n",
    "performs additional training (finetuning) in the target environment.\n",
    "\n",
    "- If `USE_PRETRAINED = True`, a pretrained checkpoint is loaded and training continues.\n",
    "- If `USE_PRETRAINED = False`, the policy is initialized from scratch.\n",
    "\n",
    "This approach significantly reduces training time while still allowing\n",
    "reward design and environment settings to influence the learned behavior.\n"
   ],
   "id": "66d2a7f92c7968a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/RL_DEMO/logs"
   ],
   "id": "583d6f8505ffaf8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "importlib.reload(go1_env)\n",
    "\n",
    "USE_PRETRAINED = True\n",
    "PRETRAINED_MODEL_PATH = \"/content/RL_DEMO/models/pretrained/final_model.zip\"\n",
    "\n",
    "# Train\n",
    "MODEL_DIR = \"models\"\n",
    "LOG_DIR = \"logs\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    go1_env.Go1MujocoEnv,\n",
    "    env_kwargs={\"prj_path\": \"/content/RL_DEMO\"},\n",
    "    n_envs=policy_cfg[\"n_envs\"],\n",
    "    seed=policy_cfg[\"seed\"],\n",
    "    vec_env_cls=SubprocVecEnv,\n",
    ")\n",
    "\n",
    "train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"{train_time}\"\n",
    "\n",
    "model_path = f\"{MODEL_DIR}/{run_name}\"\n",
    "print(\n",
    "    f\"Training on {policy_cfg['n_envs']} parallel training environments and saving models to '{model_path}'\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=policy_cfg[\"policy\"][\"n_steps\"] * policy_cfg[\"log\"][\"interval\"],  # e.g. 100_000\n",
    "    save_path=model_path,  # directory\n",
    "    name_prefix=\"model\",  # checkpoint_model_100000_steps.zip\n",
    "    save_replay_buffer=False,\n",
    "    save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    vec_env,\n",
    "    best_model_save_path=model_path,\n",
    "    log_path=LOG_DIR,\n",
    "    eval_freq=policy_cfg[\"eval_freq\"],\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "reward_logging_callback = RewardLoggingCallback()\n",
    "\n",
    "callbacks = CallbackList([\n",
    "    eval_callback,\n",
    "    checkpoint_callback,\n",
    "    reward_logging_callback,\n",
    "])\n",
    "\n",
    "if USE_PRETRAINED:\n",
    "    print(f\"Using Pretrained model from {PRETRAINED_MODEL_PATH}\")\n",
    "    model = PPO.load(path=PRETRAINED_MODEL_PATH, env=vec_env)\n",
    "    model.tensorboard_log = LOG_DIR\n",
    "else:\n",
    "    print(\"Training from Network model from scratch\")\n",
    "    model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=LOG_DIR)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=policy_cfg[\"total_timestep\"],\n",
    "    reset_num_timesteps=True,\n",
    "    progress_bar=True,\n",
    "    tb_log_name=run_name,\n",
    "    callback=callbacks,\n",
    ")\n",
    "# Save final model\n",
    "model.save(f\"{model_path}/final_model\")\n",
    "\n",
    "vec_env.close()\n",
    "\n",
    "del model\n",
    "del eval_callback\n",
    "del vec_env\n",
    "\n",
    "gc.collect()"
   ],
   "id": "6730397136b4de0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Policy Evaluation\n",
    "\n",
    "After training, the learned policy is evaluated in a single environment.\n",
    "The robot’s behavior is rendered and saved as a video for qualitative analysis.\n",
    "\n",
    "- Control frequency: 50 Hz\n",
    "- Video frame rate: 25 FPS\n",
    "- Frames are sampled periodically and saved as an MP4 file\n",
    "\n",
    "The following cell generates and displays a rollout video.\n"
   ],
   "id": "5b8fc3265c1d4e5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T13:59:40.464791Z",
     "start_time": "2026-02-07T13:59:40.253833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test\n",
    "importlib.reload(go1_env)\n",
    "model_path = \"/content/RL_DEMO/models/pretrained/final_model\"\n",
    "\n",
    "WIDTH, HEIGHT = 544, 368\n",
    "\n",
    "env = go1_env.Go1MujocoEnv(\n",
    "    prj_path=\"/content/RL_DEMO\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    camera_name=\"tracking\",\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT,\n",
    ")\n",
    "inter_frame_sleep = 0.0\n",
    "\n",
    "model = PPO.load(path=model_path, env=env, verbose=1)\n",
    "\n",
    "video_path = \"/content/rollout.mp4\"\n",
    "\n",
    "obs, _ = env.reset()\n",
    "max_time_step_s = policy_cfg[\"test\"][\"max_time_step_s\"]\n",
    "ep_len = 0\n",
    "video_fps = 25\n",
    "\n",
    "# Ctrl Hz: 50\n",
    "render_interval = 50 // video_fps\n",
    "max_steps = int(max_time_step_s * 50)\n",
    "\n",
    "writer = imageio.get_writer(\n",
    "    video_path,\n",
    "    fps=video_fps,\n",
    "    codec=\"libx264\",\n",
    "    quality=8,\n",
    "    pixelformat=\"yuv420p\",\n",
    ")\n",
    "\n",
    "frames = []\n",
    "\n",
    "while ep_len < max_steps:\n",
    "  with torch.no_grad():\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "  obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  if ep_len % render_interval == 0:\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "  ep_len += 1\n",
    "\n",
    "imageio.mimwrite(\n",
    "    video_path,\n",
    "    frames,\n",
    "    fps=video_fps,\n",
    "    codec=\"libx264\",\n",
    "    quality=8,\n",
    "    pixelformat=\"yuv420p\",\n",
    ")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Saved video to:\", video_path)\n",
    "\n",
    "display(\n",
    "    Video(\n",
    "        video_path,\n",
    "        embed=True,\n",
    "        html_attributes=\"controls autoplay loop\"\n",
    "    )\n",
    ")"
   ],
   "id": "9d220d0d164c3d9e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Test\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mimportlib\u001B[49m\u001B[38;5;241m.\u001B[39mreload(go1_env)\n\u001B[1;32m      3\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/content/RL_DEMO/models/pretrained/final_model\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m WIDTH, HEIGHT \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m544\u001B[39m, \u001B[38;5;241m368\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'importlib' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Reward Ablation Study\n",
    "\n",
    "Finally, we examine how reward design affects locomotion behavior.\n",
    "\n",
    "By loading checkpoints trained with different reward configurations\n",
    "(e.g., removing smoothness or energy penalties), we can directly observe\n",
    "how each reward term influences stability, motion smoothness, and failure modes.\n",
    "\n",
    "In the following cells, we compare rollout videos from different checkpoints\n",
    "under identical evaluation conditions.\n"
   ],
   "id": "c2495b282ff1d745"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
