{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os, sys\n",
    "os.chdir(\"/content\")\n",
    "if not os.path.isdir(\"RL_DEMO\"):\n",
    "  !git clone https://github.com/Kyu3224/RL_DEMO.git\n",
    "else:\n",
    "  print(\"Cloned Directory already exists\")\n",
    "\n",
    "os.chdir(\"/content/RL_DEMO\")\n",
    "print(\"Current Directory: \", os.getcwd())\n",
    "\n",
    "sys.path.insert(0, \"/content/RL_DEMO\")\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies\n",
    "!pip install torch numpy tensorboard gymnasium==0.29.1 protobuf==4.25.3 stable-baselines3==2.3.0 mujoco==3.1.5 imageio\n",
    "\n",
    "# import importlib\n",
    "# from src import env\n",
    "# importlib.reload(env)\n",
    "# env.print_hi()"
   ],
   "id": "34d21ce7d64e751c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import mujoco\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.mujoco import MujocoEnv\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from IPython.display import Video, display\n",
    "\n",
    "DEFAULT_CAMERA_CONFIG = {\n",
    "    \"azimuth\": 90.0,\n",
    "    \"distance\": 3.0,\n",
    "    \"elevation\": -25.0,\n",
    "    \"lookat\": np.array([0., 0., 0.]),\n",
    "    \"fixedcamid\": 0,\n",
    "    \"trackbodyid\": -1,\n",
    "    \"type\": 2,\n",
    "}\n",
    "\n",
    "class Go1MujocoEnv(MujocoEnv):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "            \"depth_array\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        model_path = Path(f\"/content/RL_DEMO/unitree_go1/scene_position.xml\")\n",
    "        MujocoEnv.__init__(\n",
    "            self,\n",
    "            model_path=model_path.absolute().as_posix(),\n",
    "            frame_skip=10,  # Perform an action every 10 frames (dt(=0.002) * 10 = 0.02 seconds -> 50hz action rate)\n",
    "            observation_space=None,  # Manually set afterwards\n",
    "            default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Update metadata to include the render FPS\n",
    "        self.metadata = {\n",
    "            \"render_modes\": [\n",
    "                \"human\",\n",
    "                \"rgb_array\",\n",
    "                \"depth_array\",\n",
    "            ],\n",
    "            \"render_fps\": 60,\n",
    "        }\n",
    "        self._last_render_time = -1.0\n",
    "        self._max_episode_time_sec = 15.0\n",
    "        self._step = 0\n",
    "\n",
    "        # Weights for the reward and cost functions\n",
    "        self.reward_weights = {\n",
    "            \"linear_vel_tracking\": 2.0,  # Was 1.0\n",
    "            \"angular_vel_tracking\": 1.0,\n",
    "            \"healthy\": 0.0,  # was 0.05\n",
    "            \"feet_airtime\": 1.0,\n",
    "        }\n",
    "        self.cost_weights = {\n",
    "            \"torque\": 0.0002,\n",
    "            \"vertical_vel\": 2.0,  # Was 1.0\n",
    "            \"xy_angular_vel\": 0.05,  # Was 0.05\n",
    "            \"action_rate\": 0.01,\n",
    "            \"joint_limit\": 10.0,\n",
    "            \"joint_velocity\": 0.01,\n",
    "            \"joint_acceleration\": 2.5e-7,\n",
    "            \"orientation\": 1.0,\n",
    "            \"collision\": 1.0,\n",
    "            \"default_joint_position\": 0.1\n",
    "        }\n",
    "\n",
    "        self._curriculum_base = 0.3\n",
    "        self._gravity_vector = np.array(self.model.opt.gravity)\n",
    "        self._default_joint_position = np.array(self.model.key_ctrl[0])\n",
    "\n",
    "        # vx (m/s), vy (m/s), wz (rad/s)\n",
    "        self._desired_velocity_min = np.array([0.5, -0.0, -0.0])\n",
    "        self._desired_velocity_max = np.array([0.5, 0.0, 0.0])\n",
    "        self._desired_velocity = self._sample_desired_vel()  # [0.5, 0.0, 0.0]\n",
    "        self._obs_scale = {\n",
    "            \"linear_velocity\": 2.0,\n",
    "            \"angular_velocity\": 0.25,\n",
    "            \"dofs_position\": 1.0,\n",
    "            \"dofs_velocity\": 0.05,\n",
    "        }\n",
    "        self._tracking_velocity_sigma = 0.25\n",
    "\n",
    "        # Metrics used to determine if the episode should be terminated\n",
    "        self._healthy_z_range = (0.22, 0.65)\n",
    "        self._healthy_pitch_range = (-np.deg2rad(10), np.deg2rad(10))\n",
    "        self._healthy_roll_range = (-np.deg2rad(10), np.deg2rad(10))\n",
    "\n",
    "        self._feet_air_time = np.zeros(4)\n",
    "        self._last_contacts = np.zeros(4)\n",
    "        self._cfrc_ext_feet_indices = [4, 7, 10, 13]  # 4:FR, 7:FL, 10:RR, 13:RL\n",
    "        self._cfrc_ext_contact_indices = [2, 3, 5, 6, 8, 9, 11, 12]\n",
    "\n",
    "        # Non-penalized degrees of freedom range of the control joints\n",
    "        dof_position_limit_multiplier = 0.9  # The % of the range that is not penalized\n",
    "        ctrl_range_offset = (\n",
    "            0.5\n",
    "            * (1 - dof_position_limit_multiplier)\n",
    "            * (\n",
    "                self.model.actuator_ctrlrange[:, 1]\n",
    "                - self.model.actuator_ctrlrange[:, 0]\n",
    "            )\n",
    "        )\n",
    "        # First value is the root joint, so we ignore it\n",
    "        self._soft_joint_range = np.copy(self.model.actuator_ctrlrange)\n",
    "        self._soft_joint_range[:, 0] += ctrl_range_offset\n",
    "        self._soft_joint_range[:, 1] -= ctrl_range_offset\n",
    "\n",
    "        self._reset_noise_scale = 0.1\n",
    "\n",
    "        # Action: 12 torque values\n",
    "        self._last_action = np.zeros(12)\n",
    "\n",
    "        self._clip_obs_threshold = 100.0\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=self._get_obs().shape, dtype=np.float64\n",
    "        )\n",
    "\n",
    "        # Feet site names to index mapping\n",
    "        # https://mujoco.readthedocs.io/en/stable/XMLreference.html#body-site\n",
    "        # https://mujoco.readthedocs.io/en/stable/APIreference/APItypes.html#mjtobj\n",
    "        feet_site = [\n",
    "            \"FR\",\n",
    "            \"FL\",\n",
    "            \"RR\",\n",
    "            \"RL\",\n",
    "        ]\n",
    "        self._feet_site_name_to_id = {\n",
    "            f: mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "            for f in feet_site\n",
    "        }\n",
    "\n",
    "        self._main_body_id = mujoco.mj_name2id(\n",
    "            self.model, mujoco.mjtObj.mjOBJ_BODY.value, \"trunk\"\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self._step += 1\n",
    "        self.do_simulation(action, self.frame_skip)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        reward, reward_info = self._calc_reward(action)\n",
    "        # TODO: Consider terminating if knees touch the ground\n",
    "        terminated = not self.is_healthy\n",
    "        truncated = self._step >= (self._max_episode_time_sec / self.dt)\n",
    "        info = {\n",
    "            \"x_position\": self.data.qpos[0],\n",
    "            \"y_position\": self.data.qpos[1],\n",
    "            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\n",
    "            **reward_info,\n",
    "        }\n",
    "\n",
    "        if self.render_mode == \"human\" and (self.data.time - self._last_render_time) > (\n",
    "            1.0 / self.metadata[\"render_fps\"]\n",
    "        ):\n",
    "            self.render()\n",
    "            self._last_render_time = self.data.time\n",
    "\n",
    "        self._last_action = action\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    @property\n",
    "    def is_healthy(self):\n",
    "        state = self.state_vector()\n",
    "        min_z, max_z = self._healthy_z_range\n",
    "        is_healthy = np.isfinite(state).all() and min_z <= state[2] <= max_z\n",
    "\n",
    "        min_roll, max_roll = self._healthy_roll_range\n",
    "        is_healthy = is_healthy and min_roll <= state[4] <= max_roll\n",
    "\n",
    "        min_pitch, max_pitch = self._healthy_pitch_range\n",
    "        is_healthy = is_healthy and min_pitch <= state[5] <= max_pitch\n",
    "\n",
    "        return is_healthy\n",
    "\n",
    "    @property\n",
    "    def projected_gravity(self):\n",
    "        w, x, y, z = self.data.qpos[3:7]\n",
    "        euler_orientation = np.array(self.euler_from_quaternion(w, x, y, z))\n",
    "        projected_gravity_not_normalized = (\n",
    "            np.dot(self._gravity_vector, euler_orientation) * euler_orientation\n",
    "        )\n",
    "        if np.linalg.norm(projected_gravity_not_normalized) == 0:\n",
    "            return projected_gravity_not_normalized\n",
    "        else:\n",
    "            return projected_gravity_not_normalized / np.linalg.norm(\n",
    "                projected_gravity_not_normalized\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def feet_contact_forces(self):\n",
    "        feet_contact_forces = self.data.cfrc_ext[self._cfrc_ext_feet_indices]\n",
    "        return np.linalg.norm(feet_contact_forces, axis=1)\n",
    "\n",
    "    ######### Positive Reward functions #########\n",
    "    @property\n",
    "    def linear_velocity_tracking_reward(self):\n",
    "        vel_sqr_error = np.sum(\n",
    "            np.square(self._desired_velocity[:2] - self.data.qvel[:2])\n",
    "        )\n",
    "        return np.exp(-vel_sqr_error / self._tracking_velocity_sigma)\n",
    "\n",
    "    @property\n",
    "    def angular_velocity_tracking_reward(self):\n",
    "        vel_sqr_error = np.square(self._desired_velocity[2] - self.data.qvel[5])\n",
    "        return np.exp(-vel_sqr_error / self._tracking_velocity_sigma)\n",
    "\n",
    "    @property\n",
    "    def feet_air_time_reward(self):\n",
    "        \"\"\"Award strides depending on their duration only when the feet makes contact with the ground\"\"\"\n",
    "        feet_contact_force_mag = self.feet_contact_forces\n",
    "        curr_contact = feet_contact_force_mag > 1.0\n",
    "        contact_filter = np.logical_or(curr_contact, self._last_contacts)\n",
    "        self._last_contacts = curr_contact\n",
    "\n",
    "        # if feet_air_time is > 0 (feet was in the air) and contact_filter detects a contact with the ground\n",
    "        # then it is the first contact of this stride\n",
    "        first_contact = (self._feet_air_time > 0.0) * contact_filter\n",
    "        self._feet_air_time += self.dt\n",
    "\n",
    "        # Award the feets that have just finished their stride (first step with contact)\n",
    "        air_time_reward = np.sum((self._feet_air_time - 1.0) * first_contact)\n",
    "        # No award if the desired velocity is very low (i.e. robot should remain stationary and feet shouldn't move)\n",
    "        air_time_reward *= np.linalg.norm(self._desired_velocity[:2]) > 0.1\n",
    "\n",
    "        # zero-out the air time for the feet that have just made contact (i.e. contact_filter==1)\n",
    "        self._feet_air_time *= ~contact_filter\n",
    "\n",
    "        return air_time_reward\n",
    "\n",
    "    @property\n",
    "    def healthy_reward(self):\n",
    "        return self.is_healthy\n",
    "\n",
    "    ######### Negative Reward functions #########\n",
    "    @property\n",
    "    def non_flat_base_cost(self):\n",
    "        # Penalize the robot for not being flat on the ground\n",
    "        return np.sum(np.square(self.projected_gravity[:2]))\n",
    "\n",
    "    @property\n",
    "    def collision_cost(self):\n",
    "        # Penalize collisions on selected bodies\n",
    "        return np.sum(\n",
    "            1.0\n",
    "            * (np.linalg.norm(self.data.cfrc_ext[self._cfrc_ext_contact_indices]) > 0.1)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def joint_limit_cost(self):\n",
    "        # Penalize the robot for joints exceeding the soft control range\n",
    "        out_of_range = (self._soft_joint_range[:, 0] - self.data.qpos[7:]).clip(\n",
    "            min=0.0\n",
    "        ) + (self.data.qpos[7:] - self._soft_joint_range[:, 1]).clip(min=0.0)\n",
    "        return np.sum(out_of_range)\n",
    "\n",
    "    @property\n",
    "    def torque_cost(self):\n",
    "        # Last 12 values are the motor torques\n",
    "        return np.sum(np.square(self.data.qfrc_actuator[-12:]))\n",
    "\n",
    "    @property\n",
    "    def vertical_velocity_cost(self):\n",
    "        return np.square(self.data.qvel[2])\n",
    "\n",
    "    @property\n",
    "    def xy_angular_velocity_cost(self):\n",
    "        return np.sum(np.square(self.data.qvel[3:5]))\n",
    "\n",
    "    def action_rate_cost(self, action):\n",
    "        return np.sum(np.square(self._last_action - action))\n",
    "\n",
    "    @property\n",
    "    def joint_velocity_cost(self):\n",
    "        return np.sum(np.square(self.data.qvel[6:]))\n",
    "\n",
    "    @property\n",
    "    def acceleration_cost(self):\n",
    "        return np.sum(np.square(self.data.qacc[6:]))\n",
    "\n",
    "    @property\n",
    "    def default_joint_position_cost(self):\n",
    "        return np.sum(np.square(self.data.qpos[7:] - self._default_joint_position))\n",
    "\n",
    "    @property\n",
    "    def smoothness_cost(self):\n",
    "        return np.sum(np.square(self.data.qpos[7:] - self._last_action))\n",
    "\n",
    "    @property\n",
    "    def curriculum_factor(self):\n",
    "        return self._curriculum_base**0.997\n",
    "\n",
    "    def _calc_reward(self, action):\n",
    "        # TODO: Add debug mode with custom Tensorboard calls for individual reward\n",
    "        #   functions to get a better sense of the contribution of each reward function\n",
    "        # TODO: Cost for thigh or calf contact with the ground\n",
    "\n",
    "        # Positive Rewards\n",
    "        linear_vel_tracking_reward = (\n",
    "            self.linear_velocity_tracking_reward\n",
    "            * self.reward_weights[\"linear_vel_tracking\"]\n",
    "        )\n",
    "        angular_vel_tracking_reward = (\n",
    "            self.angular_velocity_tracking_reward\n",
    "            * self.reward_weights[\"angular_vel_tracking\"]\n",
    "        )\n",
    "        healthy_reward = self.healthy_reward * self.reward_weights[\"healthy\"]\n",
    "        feet_air_time_reward = (\n",
    "            self.feet_air_time_reward * self.reward_weights[\"feet_airtime\"]\n",
    "        )\n",
    "        rewards = (\n",
    "            linear_vel_tracking_reward\n",
    "            + angular_vel_tracking_reward\n",
    "            + healthy_reward\n",
    "            + feet_air_time_reward\n",
    "        )\n",
    "\n",
    "        # Negative Costs\n",
    "        ctrl_cost = self.torque_cost * self.cost_weights[\"torque\"]\n",
    "        action_rate_cost = (\n",
    "            self.action_rate_cost(action) * self.cost_weights[\"action_rate\"]\n",
    "        )\n",
    "        vertical_vel_cost = (\n",
    "            self.vertical_velocity_cost * self.cost_weights[\"vertical_vel\"]\n",
    "        )\n",
    "        xy_angular_vel_cost = (\n",
    "            self.xy_angular_velocity_cost * self.cost_weights[\"xy_angular_vel\"]\n",
    "        )\n",
    "        joint_limit_cost = self.joint_limit_cost * self.cost_weights[\"joint_limit\"]\n",
    "        joint_velocity_cost = (\n",
    "            self.joint_velocity_cost * self.cost_weights[\"joint_velocity\"]\n",
    "        )\n",
    "        joint_acceleration_cost = (\n",
    "            self.acceleration_cost * self.cost_weights[\"joint_acceleration\"]\n",
    "        )\n",
    "        orientation_cost = self.non_flat_base_cost * self.cost_weights[\"orientation\"]\n",
    "        collision_cost = self.collision_cost * self.cost_weights[\"collision\"]\n",
    "        default_joint_position_cost = (\n",
    "            self.default_joint_position_cost\n",
    "            * self.cost_weights[\"default_joint_position\"]\n",
    "        )\n",
    "        costs = (\n",
    "            ctrl_cost\n",
    "            + action_rate_cost\n",
    "            + vertical_vel_cost\n",
    "            + xy_angular_vel_cost\n",
    "            + joint_limit_cost\n",
    "            + joint_acceleration_cost\n",
    "            + orientation_cost\n",
    "            + default_joint_position_cost\n",
    "        )\n",
    "\n",
    "        reward = max(0.0, rewards - costs)\n",
    "        # reward = rewards - self.curriculum_factor * costs\n",
    "        reward_info = {\n",
    "            \"linear_vel_tracking_reward\": linear_vel_tracking_reward,\n",
    "            \"reward_ctrl\": -ctrl_cost,\n",
    "            \"reward_survive\": healthy_reward,\n",
    "        }\n",
    "\n",
    "        return reward, reward_info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # The first three indices are the global x,y,z position of the trunk of the robot\n",
    "        # The second four are the quaternion representing the orientation of the robot\n",
    "        # The above seven values are ignored since they are privileged information\n",
    "        # The remaining 12 values are the joint positions\n",
    "        # The joint positions are relative to the starting position\n",
    "        dofs_position = self.data.qpos[7:].flatten() - self.model.key_qpos[0, 7:]\n",
    "\n",
    "        # The first three values are the global linear velocity of the robot\n",
    "        # The second three are the angular velocity of the robot\n",
    "        # The remaining 12 values are the joint velocities\n",
    "        velocity = self.data.qvel.flatten()\n",
    "        base_linear_velocity = velocity[:3]\n",
    "        base_angular_velocity = velocity[3:6]\n",
    "        dofs_velocity = velocity[6:]\n",
    "\n",
    "        desired_vel = self._desired_velocity\n",
    "        last_action = self._last_action\n",
    "        projected_gravity = self.projected_gravity\n",
    "\n",
    "        curr_obs = np.concatenate(\n",
    "            (\n",
    "                base_linear_velocity * self._obs_scale[\"linear_velocity\"],\n",
    "                base_angular_velocity * self._obs_scale[\"angular_velocity\"],\n",
    "                projected_gravity,\n",
    "                desired_vel * self._obs_scale[\"linear_velocity\"],\n",
    "                dofs_position * self._obs_scale[\"dofs_position\"],\n",
    "                dofs_velocity * self._obs_scale[\"dofs_velocity\"],\n",
    "                last_action,\n",
    "            )\n",
    "        ).clip(-self._clip_obs_threshold, self._clip_obs_threshold)\n",
    "\n",
    "        return curr_obs\n",
    "\n",
    "    def reset_model(self):\n",
    "        # Reset the position and control values with noise\n",
    "        self.data.qpos[:] = self.model.key_qpos[0] + self.np_random.uniform(\n",
    "            low=-self._reset_noise_scale,\n",
    "            high=self._reset_noise_scale,\n",
    "            size=self.model.nq,\n",
    "        )\n",
    "        self.data.ctrl[:] = self.model.key_ctrl[\n",
    "            0\n",
    "        ] + self._reset_noise_scale * self.np_random.standard_normal(\n",
    "            *self.data.ctrl.shape\n",
    "        )\n",
    "\n",
    "        # Reset the variables and sample a new desired velocity\n",
    "        self._desired_velocity = self._sample_desired_vel()\n",
    "        self._step = 0\n",
    "        self._last_action = np.zeros(12)\n",
    "        self._feet_air_time = np.zeros(4)\n",
    "        self._last_contacts = np.zeros(4)\n",
    "        self._last_render_time = -1.0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def _get_reset_info(self):\n",
    "        return {\n",
    "            \"x_position\": self.data.qpos[0],\n",
    "            \"y_position\": self.data.qpos[1],\n",
    "            \"distance_from_origin\": np.linalg.norm(self.data.qpos[0:2], ord=2),\n",
    "        }\n",
    "\n",
    "    def _sample_desired_vel(self):\n",
    "        desired_vel = np.random.default_rng().uniform(\n",
    "            low=self._desired_velocity_min, high=self._desired_velocity_max\n",
    "        )\n",
    "        return desired_vel\n",
    "\n",
    "    @staticmethod\n",
    "    def euler_from_quaternion(w, x, y, z):\n",
    "        \"\"\"\n",
    "        Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "        roll is rotation around x in radians (counterclockwise)\n",
    "        pitch is rotation around y in radians (counterclockwise)\n",
    "        yaw is rotation around z in radians (counterclockwise)\n",
    "        \"\"\"\n",
    "        t0 = +2.0 * (w * x + y * z)\n",
    "        t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "        roll_x = np.arctan2(t0, t1)\n",
    "\n",
    "        t2 = +2.0 * (w * y - z * x)\n",
    "        t2 = +1.0 if t2 > +1.0 else t2\n",
    "        t2 = -1.0 if t2 < -1.0 else t2\n",
    "        pitch_y = np.arcsin(t2)\n",
    "\n",
    "        t3 = +2.0 * (w * z + x * y)\n",
    "        t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "        yaw_z = np.arctan2(t3, t4)\n",
    "\n",
    "        return roll_x, pitch_y, yaw_z  # in radians"
   ],
   "id": "db38547bbadbf940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train\n",
    "MODEL_DIR = \"models\"\n",
    "LOG_DIR = \"logs\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "num_envs = 12\n",
    "seed = 42\n",
    "eval_freq = 10_000\n",
    "total_timestep = 50_000\n",
    "\n",
    "vec_env = make_vec_env(\n",
    "    Go1MujocoEnv,\n",
    "    env_kwargs={},\n",
    "    n_envs=num_envs,\n",
    "    seed=seed,\n",
    "    vec_env_cls=SubprocVecEnv,\n",
    ")\n",
    "\n",
    "train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"{train_time}\"\n",
    "\n",
    "model_path = f\"{MODEL_DIR}/{run_name}\"\n",
    "print(\n",
    "    f\"Training on {num_envs} parallel training environments and saving models to '{model_path}'\"\n",
    ")\n",
    "\n",
    "# Evaluate the model every eval_frequency for 5 episodes and save\n",
    "# it if it's improved over the previous best model.\n",
    "eval_callback = EvalCallback(\n",
    "    vec_env,\n",
    "    best_model_save_path=model_path,\n",
    "    log_path=LOG_DIR,\n",
    "    eval_freq=eval_freq,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1, tensorboard_log=LOG_DIR)\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timestep,\n",
    "    reset_num_timesteps=False,\n",
    "    progress_bar=True,\n",
    "    tb_log_name=run_name,\n",
    "    callback=eval_callback,\n",
    ")\n",
    "# Save final model\n",
    "model.save(f\"{model_path}/final_model\")"
   ],
   "id": "6730397136b4de0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_gamma(frame, gamma=0.45):\n",
    "    frame = frame.astype(np.float32) / 255.0\n",
    "    frame = np.power(frame, gamma)\n",
    "    frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "    return frame\n",
    "\n",
    "# Test\n",
    "model_path = \"/content/RL_DEMO/src/models/pretrained/final_model\"\n",
    "\n",
    "env = Go1MujocoEnv(\n",
    "    render_mode=\"rgb_array\",\n",
    "    camera_name=\"tracking\",\n",
    "    width=1080,\n",
    "    height=720,\n",
    ")\n",
    "inter_frame_sleep = 0.0\n",
    "\n",
    "model = PPO.load(path=model_path, env=env, verbose=1)\n",
    "\n",
    "num_episodes = 1\n",
    "total_reward = 0\n",
    "total_length = 0\n",
    "\n",
    "video_path = \"/content/rollout.mp4\"\n",
    "\n",
    "frames = []\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    ep_reward = 0\n",
    "    ep_len = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        frame = env.render()     # rgb_array 반환\n",
    "        # frame = apply_gamma(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "        ep_reward += reward\n",
    "        ep_len += 1\n",
    "        done = terminated or truncated\n",
    "\n",
    "    print(f\"[Episode {ep}] len={ep_len}, reward={ep_reward}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "imageio.mimsave(video_path, frames, fps=30)\n",
    "print(\"Saved video to:\", video_path)\n",
    "\n",
    "video_path = \"/content/rollout.mp4\"\n",
    "\n",
    "display(\n",
    "    Video(\n",
    "        video_path,\n",
    "        embed=True,\n",
    "        html_attributes=\"controls autoplay loop\"\n",
    "    )\n",
    ")"
   ],
   "id": "2e4b8adead29322e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
