{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Quadruped Locomotion: Reward Ablation\n",
    "\n",
    "이 노트북은 `quadruped_locomotion.ipynb` 이후에 실행되는 **reward ablation 실험용** 노트북입니다.\n",
    "\n",
    "이전 노트북:\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]\n",
    "(https://colab.research.google.com/github/DrcdKAIST/RL_DEMO/blob/master/notebooks/0.quadruped_locomotion_basic.ipynb)\n",
    "\n",
    "1. Task Reward (Velocity Tracking) 과 Termination Reward만 있는 가장 단순한 경우,\n",
    "2. 1. 의 세팅에서 기본적인 Regularization Reward (Action smoothness, Joint velocity/acceleration ...) 가 추가된 경우,\n",
    "3. 2. 의 세팅에서 Motion 및 Gait Regularization Reward 추가된 경우\n",
    "\n",
    "총 세 가지의 세팅으로 변경시켜 보며 리워드들이 어떠한 영향을 주는지 확인합니다.\n",
    "\n",
    "아래 셀부터 공통 세팅을 다시 수행합니다.\n"
   ],
   "id": "c12e4aa84eca4b3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 0. Environment Setup\n",
    "\n",
    "Colab이면 `/content`를 기준 경로로 사용하고, 그렇지 않으면 현재 작업 디렉터리를 기준 경로로 사용합니다.\n"
   ],
   "id": "2b6edef04d8958bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clone repository\n",
    "import os, sys\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Detect Colab by availability of /content or google.colab.\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "    in_colab = True\n",
    "except Exception:\n",
    "    in_colab = os.path.isdir(\"/content\")\n",
    "\n",
    "try:\n",
    "    base_dir\n",
    "except NameError:\n",
    "    base_dir = \"/content\" if in_colab else os.getcwd()\n",
    "os.chdir(base_dir)\n",
    "\n",
    "repo_dir = os.path.join(base_dir, \"RL_DEMO\")\n",
    "\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "print(f\"Repo directory: {repo_dir}\")\n",
    "\n",
    "if not os.path.isdir(repo_dir):\n",
    "  !git clone https://github.com/DrcdKAIST/RL_DEMO.git --recursive\n",
    "else:\n",
    "  print(\"Cloned Directory already exists\")\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "print(\"Current Directory: \", os.getcwd())\n",
    "\n",
    "sys.path.insert(0, repo_dir)\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n"
   ],
   "id": "1bd5f651e1f9c9e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies\n",
    "os.chdir(repo_dir + \"/thirdParty/stable_baselines3\")\n",
    "!pip install -e .[extra]\n",
    "!pip install torch numpy tensorboard gymnasium==0.29.1 mujoco==3.1.5 imageio[ffmpeg] pygments\n"
   ],
   "id": "26b38d34e14e29fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "import inspect\n",
    "\n",
    "def _render_code(code, title=\"code\", max_height=400, bg=\"transparent\", indent=16):\n",
    "    style_name = \"native\" if in_colab else \"friendly\"\n",
    "    formatter = HtmlFormatter(style=style_name, noclasses=True, linenos=\"inline\")\n",
    "    html = highlight(code, PythonLexer(), formatter)\n",
    "    css = \"\"\"\n",
    "    <style>\n",
    "    .highlight pre { margin: 0; text-align: left; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return HTML(f\"\"\"\n",
    "    {css}\n",
    "    <details>\n",
    "      <summary>{title}</summary>\n",
    "      <div style=\"margin-top:8px; margin-left:{indent}px; max-height:{max_height}px; overflow:auto; border:1px solid #ddd; padding:10px; background:{bg};\">\n",
    "        {html}\n",
    "      </div>\n",
    "    </details>\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def show_code(path, max_height=400, title=\"code\", bg=\"transparent\"):\n",
    "    code = Path(path).read_text()\n",
    "    return _render_code(code, title=title, max_height=max_height, bg=bg)\n",
    "\n",
    "def show_func(obj, max_height=400, title=\"code\", bg=\"transparent\"):\n",
    "    code = inspect.getsource(obj)\n",
    "    return _render_code(code, title=title, max_height=max_height, bg=bg)"
   ],
   "id": "6b5ef4e41896e445"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 1-1. Reward Ablation Setup - Observations\n",
    "\n",
    "실험 1,2의 경우 phase observation이 필요없기 때문에, 이를 제외하고, 실험 3의 경우 포함시켜야 하기 때문에 두가지 버전의 get_obs 함수를 준비하고이후 실험에 맞게 선택합니다.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"rsc/phase.png\">\n",
    "</div>\n"
   ],
   "id": "ad910454055fd127"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define alternate observation functions and monkey-patch (no file edits)\n",
    "# NOTE: Edit the two functions below to run ablations.\n",
    "import importlib\n",
    "import src.go1_mujoco_env as go1_mujoco_env\n",
    "importlib.reload(go1_mujoco_env)\n",
    "\n",
    "Go1MujocoEnv = go1_mujoco_env.Go1MujocoEnv\n",
    "\n",
    "# ---- Define variants ----\n",
    "\n",
    "def _get_obs_with_phase(self):\n",
    "    # TODO: update observation for ablation (with phase)\n",
    "    dofs_position = self.data.qpos[7:].flatten() - self.model.key_qpos[0, 7:]\n",
    "    velocity = self.data.qvel.flatten()\n",
    "    base_linear_velocity = velocity[:3]\n",
    "    base_angular_velocity = velocity[3:6]\n",
    "    dofs_velocity = velocity[6:]\n",
    "    desired_vel = self._desired_velocity\n",
    "    last_action = self._last_action\n",
    "    projected_gravity = self.projected_gravity\n",
    "\n",
    "    curr_obs = np.concatenate(\n",
    "        (\n",
    "            base_linear_velocity * self._obs_scale[\"linear_velocity\"],\n",
    "            base_angular_velocity * self._obs_scale[\"angular_velocity\"],\n",
    "            projected_gravity,\n",
    "            desired_vel * self._obs_scale[\"linear_velocity\"],\n",
    "            dofs_position * self._obs_scale[\"dofs_position\"],\n",
    "            dofs_velocity * self._obs_scale[\"dofs_velocity\"],\n",
    "            last_action,\n",
    "            self._phase_sin,\n",
    "        )\n",
    "    ).clip(-self._clip_obs_threshold, self._clip_obs_threshold)\n",
    "\n",
    "    return curr_obs\n",
    "\n",
    "\n",
    "def _get_obs_without_phase(self):\n",
    "    # TODO: update observation for ablation (without phase)\n",
    "    dofs_position = self.data.qpos[7:].flatten() - self.model.key_qpos[0, 7:]\n",
    "    velocity = self.data.qvel.flatten()\n",
    "    base_linear_velocity = velocity[:3]\n",
    "    base_angular_velocity = velocity[3:6]\n",
    "    dofs_velocity = velocity[6:]\n",
    "    desired_vel = self._desired_velocity\n",
    "    last_action = self._last_action\n",
    "    projected_gravity = self.projected_gravity\n",
    "\n",
    "    curr_obs = np.concatenate(\n",
    "        (\n",
    "            base_linear_velocity * self._obs_scale[\"linear_velocity\"],\n",
    "            base_angular_velocity * self._obs_scale[\"angular_velocity\"],\n",
    "            projected_gravity,\n",
    "            desired_vel * self._obs_scale[\"linear_velocity\"],\n",
    "            dofs_position * self._obs_scale[\"dofs_position\"],\n",
    "            dofs_velocity * self._obs_scale[\"dofs_velocity\"],\n",
    "            last_action,\n",
    "        )\n",
    "    ).clip(-self._clip_obs_threshold, self._clip_obs_threshold)\n",
    "\n",
    "    return curr_obs\n",
    "\n"
   ],
   "id": "89feb2608c402378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 1-2. Reward Ablation Setup\n",
    "\n",
    "학습에 필요한 함수 및 변수들을 선언합니다.\n"
   ],
   "id": "e9de7e957e8d7492"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check observation / action space\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import imageio\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from IPython.display import Video, display\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import src.go1_mujoco_env as go1_env\n",
    "\n",
    "from src.utils.reward_logging_callback import RewardLoggingCallback\n",
    "importlib.reload(go1_env)\n",
    "\n",
    "\n",
    "DEFAULT_CAMERA_CONFIG = {\n",
    "    \"azimuth\": 90.0,\n",
    "    \"distance\": 3.0,\n",
    "    \"elevation\": -25.0,\n",
    "    \"lookat\": np.array([0., 0., 0.]),\n",
    "    \"fixedcamid\": 0,\n",
    "    \"trackbodyid\": -1,\n",
    "    \"type\": 2,\n",
    "}\n",
    "\n",
    "policy_cfg_path = Path(repo_dir + \"/src/params.yaml\")\n",
    "with policy_cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    policy_cfg = yaml.safe_load(f)\n",
    "\n",
    "# colab에서 실행하기 위한 설정\n",
    "policy_cfg['n_envs'] = 12\n",
    "policy_cfg['policy']['batch_size'] = 64\n",
    "\n",
    "# Create environment (no rendering)\n",
    "env = go1_env.Go1MujocoEnv(\n",
    "    prj_path=repo_dir,\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "print(policy_cfg['n_envs'])\n",
    "print(policy_cfg['policy']['batch_size'])\n",
    "print(f\"Observation shape: {np.array(obs).shape}\\n\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "print(f\"Observation space: {env.observation_space}\\n\")\n",
    "\n",
    "def train_run(env_cfg_path=None):\n",
    "    importlib.reload(go1_env)\n",
    "\n",
    "    USE_PRETRAINED = False\n",
    "    PRETRAINED_MODEL_PATH = f\"{repo_dir}/models/pretrained3/final_model.zip\"\n",
    "\n",
    "    # Train\n",
    "    MODEL_DIR = f\"{repo_dir}/models\"\n",
    "    LOG_DIR = f\"{repo_dir}/logs\"\n",
    "\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "    class Go1MujocoEnvPatched(go1_env.Go1MujocoEnv):\n",
    "        def _get_obs(self):\n",
    "            return (_get_obs_with_phase if use_phase else _get_obs_without_phase)(self)\n",
    "\n",
    "    print(\"Use phase: \", use_phase)\n",
    "\n",
    "    vec_env = make_vec_env(\n",
    "        Go1MujocoEnvPatched,\n",
    "        env_kwargs={\"prj_path\": repo_dir, \"cfg_path\": env_cfg_path},\n",
    "        n_envs=policy_cfg[\"n_envs\"],\n",
    "        seed=policy_cfg[\"seed\"],\n",
    "        vec_env_cls=SubprocVecEnv,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"{train_time}\"\n",
    "\n",
    "    model_path = f\"{MODEL_DIR}/{run_name}\"\n",
    "    print(\n",
    "        f\"Training on {policy_cfg['n_envs']} parallel training environments and saving models to '{model_path}'\"\n",
    "    )\n",
    "\n",
    "    # 예: model_dir, model_name이 이미 정해져 있다고 가정\n",
    "    if env_cfg_path is None:\n",
    "        envs_src = Path(repo_dir) / \"src\" / \"envs.yaml\"\n",
    "    else:\n",
    "        envs_src = Path(env_cfg_path)\n",
    "    envs_dst = Path(model_path) / \"envs.yaml\"\n",
    "    envs_dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(envs_src, envs_dst)\n",
    "\n",
    "    print(\"Copied to:\", envs_dst)\n",
    "\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=policy_cfg[\"policy\"][\"n_steps\"] * policy_cfg[\"log\"][\"interval\"],  # e.g. 100_000\n",
    "        save_path=model_path,  # directory\n",
    "        name_prefix=\"model\",  # checkpoint_model_100000_steps.zip\n",
    "        save_replay_buffer=False,\n",
    "        save_vecnormalize=False,\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        vec_env,\n",
    "        best_model_save_path=model_path,\n",
    "        log_path=LOG_DIR,\n",
    "        eval_freq=policy_cfg[\"eval_freq\"],\n",
    "        n_eval_episodes=5,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "\n",
    "    reward_logging_callback = RewardLoggingCallback()\n",
    "\n",
    "    callbacks = CallbackList([\n",
    "        eval_callback,\n",
    "        checkpoint_callback,\n",
    "        reward_logging_callback,\n",
    "    ])\n",
    "\n",
    "    if USE_PRETRAINED:\n",
    "        print(f\"Using Pretrained model from {PRETRAINED_MODEL_PATH}\")\n",
    "        model = PPO.load(path=PRETRAINED_MODEL_PATH, env=vec_env,\n",
    "                    learning_rate=policy_cfg[\"policy\"][\"learning_rate\"],\n",
    "                    n_steps=policy_cfg[\"policy\"][\"n_steps\"],\n",
    "                    batch_size=policy_cfg[\"policy\"][\"batch_size\"],\n",
    "                    n_epochs=policy_cfg[\"policy\"][\"n_epochs\"],\n",
    "                    gamma=policy_cfg[\"policy\"][\"gamma\"],\n",
    "                    gae_lambda=policy_cfg[\"policy\"][\"gae_lambda\"],\n",
    "                    clip_range=policy_cfg[\"policy\"][\"clip_range\"],\n",
    "                    normalize_advantage=policy_cfg[\"policy\"][\"normalize_advantage\"],\n",
    "                    ent_coef=policy_cfg[\"policy\"][\"ent_coef\"],\n",
    "                    vf_coef=policy_cfg[\"policy\"][\"vf_coef\"],\n",
    "                    max_grad_norm=policy_cfg[\"policy\"][\"max_grad_norm\"],\n",
    "                    verbose=1,\n",
    "                    tensorboard_log=LOG_DIR)\n",
    "        model.tensorboard_log = LOG_DIR\n",
    "    else:\n",
    "        print(\"Training from Network model from scratch\")\n",
    "        model = PPO(\"MlpPolicy\",\n",
    "                    env=vec_env,\n",
    "                    learning_rate=policy_cfg[\"policy\"][\"learning_rate\"],\n",
    "                    n_steps=policy_cfg[\"policy\"][\"n_steps\"],\n",
    "                    batch_size=policy_cfg[\"policy\"][\"batch_size\"],\n",
    "                    n_epochs=policy_cfg[\"policy\"][\"n_epochs\"],\n",
    "                    gamma=policy_cfg[\"policy\"][\"gamma\"],\n",
    "                    gae_lambda=policy_cfg[\"policy\"][\"gae_lambda\"],\n",
    "                    clip_range=policy_cfg[\"policy\"][\"clip_range\"],\n",
    "                    normalize_advantage=policy_cfg[\"policy\"][\"normalize_advantage\"],\n",
    "                    ent_coef=policy_cfg[\"policy\"][\"ent_coef\"],\n",
    "                    vf_coef=policy_cfg[\"policy\"][\"vf_coef\"],\n",
    "                    max_grad_norm=policy_cfg[\"policy\"][\"max_grad_norm\"],\n",
    "                    verbose=1,\n",
    "                    tensorboard_log=LOG_DIR)\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=policy_cfg[\"total_timestep\"],\n",
    "        reset_num_timesteps=True,\n",
    "        progress_bar=True,\n",
    "        tb_log_name=run_name,\n",
    "        callback=callbacks,\n",
    "    )\n",
    "    # Save final model\n",
    "    model.save(f\"{model_path}/final_model\")\n",
    "\n",
    "    vec_env.close()\n",
    "\n",
    "    del model\n",
    "    del eval_callback\n",
    "    del vec_env\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "def eval_run():\n",
    "    # Test\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    ep_len = 0\n",
    "    importlib.reload(go1_env)\n",
    "    model_path = f\"{repo_dir}/models/{model_name}/final_model.zip\"\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    WIDTH, HEIGHT = 320, 240\n",
    "\n",
    "    # Set a fixed command for testing [vx (m/s), vy (m/s), wz (rad/s)]\n",
    "    given_command = [0.8, 0.0, 0.0]\n",
    "\n",
    "    go1_env.Go1MujocoEnv._get_obs = _get_obs_with_phase if use_phase else _get_obs_without_phase\n",
    "\n",
    "    env = go1_env.Go1MujocoEnv(\n",
    "        prj_path=f\"{repo_dir}\",\n",
    "        cfg_path=f\"{repo_dir}/models/{model_name}/envs.yaml\",\n",
    "        given_command=given_command,  # Use fixed command instead of random\n",
    "        render_mode=\"rgb_array\",\n",
    "        camera_name=\"tracking\",\n",
    "        width=WIDTH,\n",
    "        height=HEIGHT,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    env._reset_noise_scale = 0.05 # reduce intial random noise\n",
    "\n",
    "    inter_frame_sleep = 0.0\n",
    "\n",
    "    model = PPO.load(path=model_path, env=env, verbose=1)\n",
    "\n",
    "    video_path = f\"{repo_dir}/../rollout_{model_name}.mp4\"\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    max_time_step_s = policy_cfg[\"test\"][\"max_time_step_s\"]\n",
    "    ep_len = 0\n",
    "    t_render = 0.0\n",
    "    n_render = 0\n",
    "    last_render = 0.0\n",
    "    start = time.perf_counter()\n",
    "    video_fps = 10\n",
    "\n",
    "    # Ctrl Hz: 50\n",
    "    render_interval = 50 // video_fps\n",
    "    max_steps = int(max_time_step_s * 50)\n",
    "\n",
    "    writer = imageio.get_writer(\n",
    "        video_path,\n",
    "        fps=video_fps,\n",
    "        codec=\"libx264\",\n",
    "        quality=8,\n",
    "        pixelformat=\"yuv420p\",\n",
    "    )\n",
    "\n",
    "    frames = []\n",
    "    pbar = tqdm(total=max_steps, desc=\"rollout\", unit=\"step\", dynamic_ncols=True)\n",
    "\n",
    "    print(\"max time:\", max_time_step_s)\n",
    "    print(\"max step: \", max_steps)\n",
    "\n",
    "    while ep_len < max_steps:\n",
    "      with torch.no_grad():\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "      obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      if ep_len % render_interval == 0:\n",
    "        t0 = time.perf_counter()\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        current_dur = time.perf_counter() - t0\n",
    "        t_render += current_dur\n",
    "        n_render += 1\n",
    "\n",
    "      # ---- status bar 업데이트 ----\n",
    "      elapsed = time.perf_counter() - start\n",
    "      steps_per_sec = ep_len / max(elapsed, 1e-9)\n",
    "      avg_render = (t_render / n_render) if n_render else 0.0\n",
    "\n",
    "      pbar.set_postfix({\n",
    "          \"steps/s\": f\"{steps_per_sec:6.1f}\",\n",
    "          \"renders\": n_render,\n",
    "          \"r_last(s)\": f\"{last_render:5.3f}\",\n",
    "          \"r_avg(s)\": f\"{avg_render:5.3f}\",\n",
    "      })\n",
    "      pbar.update(1)\n",
    "\n",
    "\n",
    "      ep_len += 1\n",
    "\n",
    "    imageio.mimwrite(\n",
    "        video_path,\n",
    "        frames,\n",
    "        fps=video_fps,\n",
    "        codec=\"libx264\",\n",
    "        quality=8,\n",
    "        pixelformat=\"yuv420p\",\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"avg render sec:\", t_render / max(n_render, 1))\n",
    "    print(\"Saved video to:\", video_path)\n",
    "\n",
    "    return video_path\n"
   ],
   "id": "79af506d8bd714e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1-3. 실험 선택\n",
    "\n",
    "진행할 실험 블럭을 실행합니다.\n",
    "\n",
    "사용하는 Reward, Reward coef등을 각 실험에 맞게 조정합니다. 미리 체크포인트 안쪽에 사용했던 세팅 yaml 파일을 넣어놓았습니다. 이를 불러와 src/envs.yaml 을 대체합니다.\n",
    "\n",
    "만약 새로운 reward func를 추가하거나 변경하고 싶다면, src/mdp/reward.py, go1_mujoco_env.py 내부의 _get_reward() 및 src/envs.yaml을 수정합니다."
   ],
   "id": "dbfc9b351c72b8d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 실험 1 : Task Reward (Velocity Tracking) 과 Termination Reward만 있는 가장 단순한 경우",
   "id": "bb483c44b01fc30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 실험 1\n",
    "use_phase = False\n",
    "model_name = \"pretrained\"\n",
    "env_cfg_path = f\"{repo_dir}/models/pretrained/envs.yaml\"\n",
    "\n",
    "show_code(env_cfg_path, title= model_name + \" yaml\", max_height=800)\n"
   ],
   "id": "466ee11465127a7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 실험 2 : 실험 1의 세팅에서 기본적인 Regularization Reward가 추가된 경우",
   "id": "84d1283cbc31844c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "use_phase = False\n",
    "model_name = \"pretrained2\"\n",
    "env_cfg_path = f\"{repo_dir}/models/pretrained2/envs.yaml\"\n",
    "\n",
    "show_code(env_cfg_path, title= model_name + \" yaml\", max_height=800)"
   ],
   "id": "17e5645563a508e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 실험 3: 실험 2의 세팅에서 Motion 및 Gait Regularization Reward 추가된 경우",
   "id": "21ebfefaa243395b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "use_phase = True\n",
    "model_name = \"pretrained3\"\n",
    "env_cfg_path = f\"{repo_dir}/models/pretrained3/envs.yaml\"\n",
    "\n",
    "show_code(env_cfg_path, title= model_name + \" yaml\", max_height=800)"
   ],
   "id": "4963931493687fbf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 2. Training\n"
   ],
   "id": "13cb47c2d1ba13ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_run(env_cfg_path=env_cfg_path)",
   "id": "f6932f96d939f337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## 3. Evaluation\n",
    "\n",
    "학습한 policy를 확인합니다.\n",
    "\n",
    "실습 시간에는 아래 Pretrained된 정책을 이용하여 학습 결과를 확인합니다."
   ],
   "id": "605aace6cbba60c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "video_path = eval_run()",
   "id": "80289c3471189105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "display(\n",
    "    Video(\n",
    "        video_path,\n",
    "        embed=True,\n",
    "        html_attributes=\"controls autoplay loop\"\n",
    "    )\n",
    ")"
   ],
   "id": "36c30ead7cf92da8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. 결과 비교\n",
    "\n",
    "각 실험에 대하여 Training 및 Evaluation을 진행 한 뒤, 결과를 비교합니다.\n",
    "\n",
    "실습 시간에는 미리 학습된 pretrained model로 Evaluation만 진행한 후, 결과를 비교합니다.\n"
   ],
   "id": "a3dbdff714c3a0c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "\n",
    "def video_embed(path, caption, width=320, autoplay=True):\n",
    "    data = Path(path).read_bytes()\n",
    "    b64 = base64.b64encode(data).decode(\"ascii\")\n",
    "    attrs = \"controls\"\n",
    "    if autoplay:\n",
    "        attrs += \" autoplay muted loop\"\n",
    "    return f\"\"\"\n",
    "    <figure style=\"margin:0; text-align:center;\">\n",
    "      <video {attrs} width=\"{width}\">\n",
    "        <source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\">\n",
    "      </video>\n",
    "      <figcaption style=\"margin-top:6px; font-size:12px;\">{caption}</figcaption>\n",
    "    </figure>\n",
    "    \"\"\"\n",
    "\n",
    "videos = [\n",
    "    (f\"{repo_dir}/../rollout_pretrained.mp4\",  \"Pretrained 1: (Task + Termination)\"),\n",
    "    (f\"{repo_dir}/../rollout_pretrained2.mp4\", \"Pretrained 2: (Pre 1 + Basic Regularization)\"),\n",
    "    (f\"{repo_dir}/../rollout_pretrained3.mp4\", \"Pretrained 3: (Pre 2 + Motion / Gait Regularization)\"),\n",
    "]\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<div style=\"display:flex; gap:12px;\">\n",
    "  {''.join([video_embed(p, cap, width=320) for p, cap in videos])}\n",
    "</div>\n",
    "\"\"\")"
   ],
   "id": "bddf877ea0d9d264"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e226afe62dc13cdc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
